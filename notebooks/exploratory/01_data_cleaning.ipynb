{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table_of_contents'></a>\n",
    "\n",
    "0. [Import libraries](#imports)\n",
    "1. [Import data](#import_data)\n",
    "2. [Initial Cleaning](#initial_cleaning)\n",
    "3. [Price and Quantity Cleaning](#price_and_quantity_cleaning)\n",
    "4. [Data Checks](#data_checks)\n",
    "5. [Export cleaned data](#export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries <a id='imports'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "mpl.rcParams['figure.dpi']= 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import raw data <a id='import_data'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "data_dir = Path('../../data')\n",
    "file_path = data_dir / 'interim' / '05052024_roast_review_openrefine.csv'\n",
    "df_raw = pd.read_csv(file_path)\n",
    "\n",
    "display(df_raw.info())\n",
    "display(df_raw.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Cleaning <a id='initial_cleaning'></a>\n",
    "[Back to top](#table_of_contents)\n",
    "\n",
    "First step is to do some of the basic data checks and cleanup. This includes dropping columns that are not needed, setting datatypes, renaming columns,\n",
    "combining columns, cleaning up strings, and creating new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tweak_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Initial data tweaks\"\"\"\n",
    "    return(\n",
    "        df\n",
    "        .assign(review_date = lambda df_: pd.to_datetime(df_['review_date'], format=\"%B %Y\"),\n",
    "                # Combing acidity and acidity/structure into one column, they are the same \n",
    "                # field but names used in reviews changed at one point\n",
    "                acidity = lambda df_: df_['acidity'].fillna(df_['acidity/structure']),\n",
    "                # Split the agtron column into one for external bean agtron data and ground \n",
    "                # bean agtron data\n",
    "                agtron_external=lambda df_: pd.to_numeric(df_['agtron']\n",
    "                                    .str.split('/', expand=True)[0]\n",
    "                                    .str.strip(), errors='coerce'),\n",
    "                agtron_ground = lambda df_: pd.to_numeric(df_['agtron']\n",
    "                                        .str.split('/', expand=True)[1]\n",
    "                                        .str.strip(), errors='coerce'\n",
    "                                        )\n",
    "        )\n",
    "        .dropna(subset=['agtron_external', 'agtron_ground', 'acidity',\n",
    "                        'review_date', 'est_price', 'coffee_origin',\n",
    "                        'aroma', 'roast_level', 'aftertaste',]\n",
    "        )\n",
    "        .drop(columns=['with_milk', 'acidity/structure',])\n",
    "        .astype({'acidity': 'float'})\n",
    "        .replace('', np.nan)\n",
    "        .replace('United States of America', 'USA')\n",
    "        # Agtron values must be equalt to or below 100, some entries on website have typos \n",
    "        .loc[lambda df_: (df_['agtron_external'] <= 100) & (df_['agtron_ground'] <= 100), :]\n",
    "        # Run str.strip on every string column\n",
    "        .applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    )\n",
    "\n",
    "df = df_raw.pipe(tweak_df)\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Price and Quantity Cleaning <a id='price_and_quantity_cleaning'></a>\n",
    "[Back to top](#table_of_contents)\n",
    "\n",
    "The `est_price` column contains information on price, currency, quantity and its unit of measurement. We will split these up into their own columns and clean them up.\n",
    "\n",
    "We split on the \"/\" character to create one column with price and currency information and another with quantity and unit information and then further process these.\n",
    "\n",
    "The quantities have to be cleaned so they contain a single representation for each unit and so unecessary punctuation and parentheses are removed. We filter the dataset to remove all products that came in units of cans, boxes, capusles, pods, etc. We will only concern ourselves with coffee sold in bags or bulk, ground or whole.\n",
    "\n",
    "Regular expressions are used to separate the numerical and non-numerical characters from the quantity and price columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_quantity_split(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    price_quantity = (\n",
    "        df\n",
    "        # Split est_price into columns for price and quantity\n",
    "        .est_price.str.split(\"/\", n=1, expand=True)\n",
    "        # Remove any commas from the price and quantity columns\n",
    "        .replace(',', '', regex=True)\n",
    "        .rename(columns={0: 'price', 1: 'quantity'})\n",
    "        .assign(quantity = lambda df_: (df_['quantity']\n",
    "                                        # Remove parentheses and anything inside them\n",
    "                                        .str.replace(r\"\\(.*?\\)\", \"\", regex=True)\n",
    "                                        # Remove anything after a semicolon. This is usually a note, or deal price.\n",
    "                                        .str.replace(r\";.*\", \"\", regex=True)\n",
    "                                        # Standardize units\n",
    "                                        .str.replace(r\".g$\", \" grams\", regex=True)\n",
    "                                        .str.replace(r\"\\sg$\", \"grams\", regex=True)\n",
    "                                        .str.replace(r\"pound$\", \"1 pounds\", regex=True)\n",
    "                                        .str.replace(r\"oz|onces|ounce$|ounces\\*\", \"ounces\", regex=True)\n",
    "                                        # Remove \"online\" from any quantity\n",
    "                                        .str.replace(\"online\", \"\")\n",
    "                                        .str.strip()\n",
    "                                        )\n",
    "            )\n",
    "        .dropna()\n",
    "        # Remove rows where coffee is sold in a can, box, pouch, packet, or tin\n",
    "        .loc[lambda df_: ~df_['quantity'].str.contains('can|box|capsules|K-|cups|pods|pouch|packet|tin'), :]\n",
    "        # Split quantity into value and unit, and split price into value and currency\n",
    "        .assign(quantity_value = lambda df_: (df_['quantity']\n",
    "                                              .str.extract(r'(\\d+)')\n",
    "                                              .astype(float)\n",
    "                                              ),\n",
    "                quantity_unit = lambda df_: (df_['quantity']\n",
    "                                             .str.replace(r\"(\\d+)\", \"\", regex=True)\n",
    "                                             .replace(\"\\.\", \"\", regex=True)\n",
    "                                             .str.strip()\n",
    "                                             .mask(lambda s: s == 'g', 'grams')\n",
    "                                             .str.strip()\n",
    "                                             ),\n",
    "                price_value = lambda df_: (df_['price']\n",
    "                                           .str.extract(r'(\\d+\\.\\d+|\\d+)')\n",
    "                                           .astype(float)\n",
    "                                           ),\n",
    "                price_currency = lambda df_: (df_['price']\n",
    "                                              .str.replace(\",\", \"\")\n",
    "                                              .str.replace(r'(\\d+\\.\\d+|\\d+)', '', regex=True)\n",
    "                                              .str.strip()\n",
    "                                              )\n",
    "                )\n",
    "        # Drop the original price and quantity columns\n",
    "        .drop(columns=['price', 'quantity'])\n",
    "    )\n",
    "    # Merge the price_quantity DataFrame with the original DataFrame\n",
    "    return df.merge(price_quantity, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "df = (df_raw\n",
    "      .pipe(tweak_df)\n",
    "      .pipe(price_quantity_split)\n",
    "      )\n",
    "\n",
    "display(df.info())\n",
    "display(df.loc[:, ['price_value', 'quantity_value']].describe())\n",
    "display(df['price_currency'].value_counts())   \n",
    "display(df['quantity_unit'].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Currencies\n",
    "\n",
    "Normalize the currency column to contain a standardized set of currency symbols. We will use the ISO 4217 codes to make it easier to get foreign exchange data from an external API later on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_currency(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize currencies to ISO 4217 codes.\"\"\"\n",
    "    price_currency = (\n",
    "        df.price_currency\n",
    "        .str.upper()\n",
    "        .str.replace(r'^\\$$', 'USD', regex=True)\n",
    "        .str.replace('PRICE: $', 'USD')\n",
    "        .str.replace('$', '')\n",
    "        .str.replace('#', 'GBP')\n",
    "        .str.replace('¥', 'JPY')\n",
    "        .str.replace('£', 'GBP')\n",
    "        .str.replace('POUND', 'GBP')\n",
    "        .str.replace('PESOS', 'MXN')\n",
    "        .str.replace('RMB', 'CNY')\n",
    "        .str.strip()\n",
    "        .mask(lambda s: s == \"US\", \"USD\")\n",
    "        .mask(lambda s: s == ' ', \"USD\")\n",
    "        .mask(lambda s: s == 'E', 'EUR')\n",
    "        .mask(lambda s: s == 'NTD', 'TWD')\n",
    "        .mask(lambda s: s == 'NT', 'TWD')\n",
    "        .mask(lambda s: s == '', 'USD')\n",
    "        .mask(lambda s: s == 'HK', 'HKD')\n",
    "        .str.strip()\n",
    "    )\n",
    "    return df.assign(price_currency=price_currency)\n",
    "\n",
    "df = df_raw.pipe(tweak_df).pipe(price_quantity_split).pipe(clean_currency)\n",
    "df.loc[:, [\"est_price\", \"price_currency\"]].groupby('price_currency').sample(3, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting prices to 2024 USD\n",
    "\n",
    "1. Convert price to USD using historical exchange rates\n",
    "2. Adjust price to 2024 USD using BLS consumer price index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_row(row):\n",
    "    date = row['review_date'].strftime('%Y-%m-%d')\n",
    "    currency = row['price_currency']\n",
    "    price = row['price_value']\n",
    "    if currency == 'USD':\n",
    "        return price\n",
    "    else:\n",
    "        return np.round(price / exchange_rates[date][currency], 2)\n",
    "    \n",
    "def convert_to_usd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['price_value_usd_hist'] = df.apply(convert_row, axis=1)\n",
    "    return df\n",
    "    \n",
    "# Read in exchange rates\n",
    "with open(data_dir / 'external' / 'openex_exchange_rates.json') as f:\n",
    "    exchange_rates = json.load(f)\n",
    "\n",
    "df = (df_raw\n",
    "      .pipe(tweak_df)\n",
    "      .pipe(price_quantity_split)\n",
    "      .pipe(clean_currency)\n",
    "      .pipe(convert_to_usd)\n",
    "      )\n",
    "\n",
    "(\n",
    "    df\n",
    "    .loc[:, ['price_value','price_currency', 'price_value_usd_hist']]\n",
    "    .groupby('price_currency')\n",
    ").sample(3, replace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_cpi(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads and transforms the CPI data.\"\"\"\n",
    "    MONTH_MAP = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        cpi = pd.read_csv(file_path, usecols=['Year',\n",
    "                                              'Jan', 'Feb', 'Mar',\n",
    "                                              'Apr', 'May', 'Jun',\n",
    "                                              'Jul', 'Aug', 'Sep',\n",
    "                                              'Oct', 'Nov', 'Dec'\n",
    "                                              ]\n",
    "                          )\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"CPI file is not found in the specified directory.\")\n",
    "\n",
    "    return (cpi\n",
    "            .melt(id_vars='Year', var_name='Month', value_name='cpi')\n",
    "            .assign(Month=lambda x: x['Month'].map(MONTH_MAP),\n",
    "                    date=lambda x: pd.to_datetime(x[['Year', 'Month']].assign(day=1)))\n",
    "            .dropna()\n",
    "            .drop(columns=['Year', 'Month'])\n",
    "            .rename(columns={'cpi': 'consumer_price_index'})\n",
    "            .sort_values('date')\n",
    "            .reset_index(drop=True)\n",
    "           )\n",
    "    \n",
    "def create_cpi_adjusted_price(df: pd.DataFrame, file_path: Path, date: str='2024-01-01') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adjusts historical price data to 2024 prices using CPI data.\n",
    "    \"\"\"\n",
    "    cpi = load_transform_cpi(file_path)\n",
    "    cpi_baseline = cpi.loc[cpi['date'] == date, 'consumer_price_index'].values[0]\n",
    "    \n",
    "    return (df\n",
    "            .merge(cpi, left_on=\"review_date\", right_on=\"date\")\n",
    "            .drop(columns='date')\n",
    "            .assign(price_usd_adj_2024=lambda df_: np.round(\n",
    "                df_['price_value_usd_hist'] * cpi_baseline / df_['consumer_price_index'], 2)\n",
    "                    )\n",
    "            )\n",
    "\n",
    "data_dir = Path('../../data')\n",
    "cpi_path = data_dir / 'external' / 'consumer_price_index.csv'\n",
    "\n",
    "df = (df_raw\n",
    "      .pipe(tweak_df)\n",
    "      .pipe(price_quantity_split)\n",
    "      .pipe(clean_currency)\n",
    "      .pipe(convert_to_usd)\n",
    "      .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "      )\n",
    "\n",
    "df.sample(3)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.assign(price_diff=lambda df_: (df_['price_usd_adj_2024'] - df_['price_value_usd_hist'])/df_['price_usd_adj_2024'])\n",
    ").plot(x='review_date',\n",
    "       y='price_diff',\n",
    "       title='% Price difference between adjusted and historical prices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting quantities to lbs\n",
    "\n",
    "Create a normalized quantity column that converts all quantities to lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lbs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    to_lbs_conversion = {\"ounces\": 1/16, \"pounds\":1, \"kilogram\": 2.20462, \"grams\": 0.00220462}\n",
    "    df['quantity_in_lbs'] = np.round(df['quantity_value'] * df['quantity_unit'].map(to_lbs_conversion), 2)\n",
    "    return df\n",
    "\n",
    "df =(df_raw\n",
    "     .pipe(tweak_df)\n",
    "     .pipe(price_quantity_split)\n",
    "     .pipe(clean_currency)\n",
    "     .pipe(convert_to_usd)\n",
    "     .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "     .pipe(convert_to_lbs)\n",
    ")\n",
    "\n",
    "(df.loc[:, ['quantity_value', 'quantity_unit', 'quantity_in_lbs']]\n",
    "        .groupby(\"quantity_unit\")\n",
    "        .sample(3, replace=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for price per pound\n",
    "def price_per_lbs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['price_usd_adj_2024_per_lb'] = np.round(df['price_usd_adj_2024'] / df['quantity_in_lbs'], 2)\n",
    "    return df\n",
    "\n",
    "df = (df_raw\n",
    "      .pipe(tweak_df)\n",
    "      .pipe(price_quantity_split)\n",
    "      .pipe(clean_currency)\n",
    "      .pipe(convert_to_usd)\n",
    "      .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "      .pipe(convert_to_lbs)\n",
    "      .pipe(price_per_lbs)\n",
    ")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of US states\n",
    "us_states_and_territories= [\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
    "    'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "    'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
    "    'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
    "    'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "    'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "    'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont',\n",
    "    'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming',\n",
    "    'District of Columbia', 'Puerto Rico'\n",
    "]\n",
    "\n",
    "def create_us_state(row):\n",
    "    if row['territorial_entity_2'] in us_states_and_territories:\n",
    "        return row['territorial_entity_2']\n",
    "    elif row['territorial_entity_1'] in us_states_and_territories:\n",
    "        return row['territorial_entity_1']\n",
    "    elif row['og_roaster_location'].split(\",\")[-1].strip() in us_states_and_territories:\n",
    "        return row['og_roaster_location'].split(\",\")[-1].strip()\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def create_county_and_state_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['roaster_county'] = np.where(df['territorial_entity_1'].str.contains('County', na=False),\n",
    "                                    df['territorial_entity_1'],\n",
    "                                    np.nan)\n",
    "    df['roaster_us_state'] = df.apply(create_us_state, axis=1)\n",
    "    return df\n",
    "\n",
    "df = (df_raw\n",
    "      .pipe(tweak_df)\n",
    "      .pipe(price_quantity_split)\n",
    "      .pipe(clean_currency)\n",
    "      .pipe(convert_to_usd)\n",
    "      .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "      .pipe(convert_to_lbs)\n",
    "      .pipe(price_per_lbs)\n",
    "      .pipe(create_county_and_state_columns) \n",
    ")\n",
    "\n",
    "display(df.loc[df['roaster_country'] == 'USA', ['roaster_country', 'roaster_us_state', 'roaster_county']].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Checks <a id='data_checks'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df_raw\n",
    "        .pipe(tweak_df)\n",
    "        .pipe(price_quantity_split)\n",
    "        .pipe(clean_currency)\n",
    "        .pipe(convert_to_usd)\n",
    "        .pipe(create_cpi_adjusted_price, file_path=cpi_path)\n",
    "        .pipe(convert_to_lbs)\n",
    "        .pipe(price_per_lbs)\n",
    "        .pipe(create_county_and_state_columns)\n",
    "    )\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['number']).drop(columns=['price_value'], axis=1)\n",
    "len(df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(df_numeric.columns):\n",
    "    df[col].plot(kind='hist', ax=ax[i//5, i%5], title=col, bins=20, edgecolor='black', alpha=0.7)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_usd_adj_2024_per_lb'] < 200].price_usd_adj_2024_per_lb.hist(cumulative=True,\n",
    "                                                                           density=True,\n",
    "                                                                           edgecolor='black',\n",
    "                                                                           alpha=0.7)\n",
    "plt.title(\"Cumulative Histogram Price $USD/lbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking countries\n",
    "display(df.roaster_country.sort_values().unique())\n",
    "display(df.coffee_origin_country.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking roast level\n",
    "df.roast_level.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Export cleaned data <a id='export_data'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = data_dir / 'processed' / '05052024_roast_review_cleaned.csv'\n",
    "df.to_csv(fout, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee-review-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
